{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrame"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Spark 1.0 provided RDDs as its sole data API, supporting transformations and actions that can be used to control how the data is processed in a pipeline. The main advantage of RDDs is that they are simple and well understood because they deal with concrete classes, providing a familiar object-oriented programming style. \n",
    "\n",
    "The main disadvantage to RDDs is that they donâ€™t perform particularly well. RDDs serialize all individual object which causes overhead. Other disadvantages are that the code that uses RDD can become less clear in the absense of concrete labels for columns, the use of RDDs demands lower level coding compared to more declarative languages (e.g. SQL), and external libraries rarely provide direct support for RDDs.\n",
    "\n",
    "In Spark 1.3 DataFrames are supported, which allow for faster processing by allowing objects to be kept in memory instead of serializing everything to disk for every step. DataFrames are the data structure that is used by the very well supported `pandas` library, allowing to use external libraries that support pandas DataFrame in Spark. DataFrames provide a more table-like perspective on data, that can be operated with its own API or declarative languages such as SQL. The engine translates SQL queries to DataFrame operations, therefore, most SQL queries are as efficient as their DataFrame equivalent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = sc.parallelize([('M', 25), \n",
    "                       ('M', 20), \n",
    "                       ('M', 30), \n",
    "                       ('F', 25),\n",
    "                       ('F', 20),\n",
    "                       ('M', 30)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a DataFrame requires a dataset (e.g. RDD, List), and to provide a schema. In the most simple case, Spark will infer the correct datatypes from the dataset, so only the names for the columns are required.\n",
    "\n",
    "Alternatively, data can also be read directly into DataFrames, for instance by using `pandas` or `parquet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The structure of the dataframe is DataFrame[gender: string, age: bigint]\n",
      "+------+---+\n",
      "|gender|age|\n",
      "+------+---+\n",
      "|     M| 25|\n",
      "|     M| 20|\n",
      "|     M| 30|\n",
      "|     F| 25|\n",
      "|     F| 20|\n",
      "|     M| 30|\n",
      "+------+---+\n",
      "\n",
      "the number of rows in the dataframe is 6\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(data, ['gender', 'age'])\n",
    "print(\"The structure of the dataframe is {}\".format(df))\n",
    "#show the result of the dataframe\n",
    "df.show()\n",
    "print(\"the number of rows in the dataframe is {}\".format(df.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different from RDDs, the columns are labelled which allows for clear code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|gender|age|\n",
      "+------+---+\n",
      "|     M| 25|\n",
      "|     M| 20|\n",
      "|     M| 30|\n",
      "|     M| 30|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.gender == 'M').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|gender|count|\n",
      "+------+-----+\n",
      "|     F|    2|\n",
      "|     M|    4|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(df.gender).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|gender|max(age)|\n",
      "+------+--------+\n",
      "|     F|      25|\n",
      "|     M|      30|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(df.gender).max(\"age\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+--------+--------+\n",
      "|gender|total|min(age)|max(age)|\n",
      "+------+-----+--------+--------+\n",
      "|     F|   45|      20|      25|\n",
      "|     M|  105|      20|      30|\n",
      "+------+-----+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import mean, sum, max, min, col\n",
    "df1 = df.groupBy(df.gender)\n",
    "df1.agg(sum(\"age\").alias(\"total\"), min(\"age\"), max(\"age\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some file formats (e.g. .csv), there are also readers that can read the data directly as DataFrame. In this example, we will use a dataset with airline on-time statistics and delay causes. The spark.csv reader can use the column labels that are in the file. The option inferSchema means that datatypes are correctly inferred. This is actually an expensive operation for large data volumes, alternatively you can manually specify the schema, but in this tutorial, we will stick with inferSchema. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = '../data/2008.csv.bz2'\n",
    "if not os.path.exists(filename):\n",
    "    import urllib.request\n",
    "    urllib.request.urlretrieve (\"http://stat-computing.org/dataexpo/2009/2008.csv.bz2\", \\\n",
    "                                filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = sqlContext.read.format(\"com.databricks.spark.csv\").\\\n",
    "    options(header=\"true\", inferSchema = \"true\").load(filename)\n",
    "f.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can very easily compute the average delays for departure and arrival per flight number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "averagedelays = f.groupBy(f.FlightNum).agg(mean(\"DepDelay\"), mean(\"ArrDelay\"))\n",
    "averagedelays.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## caching\n",
    "\n",
    "When you need intermediate results more often, you can cache them to get faster access. You will experience this when you repeat the show() command several times, the first time is slow, after that it is fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "averagedelays.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "averagedelays.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
