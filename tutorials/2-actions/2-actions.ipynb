{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike **transformations** which produce RDDs, **action** functions produce a value back to the caller. In Spark, transformations are *lazy* evaluated, also known as a call-by-need evaluation strategy that delays the evaluation of an expression until its value is needed. **Lazy evaluation** allows for more efficient processing, avoiding processing of experessions that are not needed for the requested value, optimally combining multiple transformations, and sharing intermediate results when possible. As a rule, transformations describe a lazy process flow, and an **action** may trigger (part) of the flow to be executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "elements = sc.parallelize([\"Peter\", \"Mike\", \"James\", \"John\", \"Luke\", \"Phil\", \"Mike\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **collect()** action return all the elements of the RDD as a Python list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Peter', 'Mike', 'James', 'John', 'Luke', 'Phil', 'Mike']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elements.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For inspection, the **first()** action return the first element as a value, and the **take(n)** action returns the first n elements as a Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Peter'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elements.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Peter', 'Mike', 'James', 'John', 'Luke']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elements.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the **count()** action we can see the size of the RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elements.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The **reduce(f)** action can be used with a custom function f to aggregate the values. The function f must accept two values and produce one. For example, we can use a max function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Phil'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elements.reduce(lambda x, y: max(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The **foreach(f)** function causes the function f to be used on every element in the RDD. The function f can for instance be an accumulator or store the results externally. The use of foreach falls outside the scope of this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **saveAsTextFile(directory)** action, stores the RRD in the given directory. If the filesystem is distributed (e.g. the Hadoop Distributed FS), the data is likely to be stored local to the nodes that hold the data, and replicated to avoid loss and to improve accessibility. But even using a local filesystem, the data is often stored in numbered parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "if os.path.isdir('elements'): # must remove if exists\n",
    "    shutil.rmtree('elements')\n",
    "elements.saveAsTextFile('elements')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading back a texfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Peter', 'Mike', 'James', 'John', 'Luke', 'Phil', 'Mike']\n"
     ]
    }
   ],
   "source": [
    "elements1 = sc.textFile('elements')\n",
    "print(elements1.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All transformations in Spark are lazy, in that they do not compute their results right away. Instead, they just remember the transformations applied to some base dataset (e.g. a file). The transformations are only computed when an action requires a result to be returned to the driver program. This design enables Spark to run more efficiently. For example, we can realize that a dataset created through map will be used in a reduce and return only the result of the reduce to the driver, rather than the larger mapped dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, each transformed RDD may be recomputed each time you run an action on it. However, you may also persist an RDD in memory using the **persist()** method, in which case Spark will keep the elements around on the cluster for much faster access the next time you query it. A special case of **persist** is to use **cache** which forces the RDD to remain in memory. There is also support for persisting RDDs on disk, or replicated across multiple nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StorageLevel(False, False, False, False, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elements1.getStorageLevel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StorageLevel(False, True, False, False, 1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elements1.cache()\n",
    "elements1.getStorageLevel() # lists: disk, memory, offheap, deserialized, #replications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Top ** \n",
    "\n",
    "retrieves the top elements according to a given function.\n",
    "\n",
    "First, let's setup an RDD on the babynames collection, like in the previous tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "babyrddprimitive = sc.textFile(\"../data/babynames.csv\")\n",
    "firstline = babyrddprimitive.first()\n",
    "babyrddnofirstline = babyrddprimitive.filter(lambda x: x != firstline)\n",
    "babyrdd = babyrddnofirstline.map(lambda x: x.split(','))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The use `top` to find the 5 most common babynames in 2013."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['2013', 'DAVID', 'KINGS', 'M', '272'],\n",
       " ['2013', 'JAYDEN', 'KINGS', 'M', '268'],\n",
       " ['2013', 'MOSHE', 'KINGS', 'M', '219'],\n",
       " ['2013', 'JAYDEN', 'QUEENS', 'M', '219'],\n",
       " ['2013', 'ETHAN', 'QUEENS', 'M', '216']]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "babyrdd.filter(lambda x: x[0] == '2013').top(5, lambda x: int(x[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
