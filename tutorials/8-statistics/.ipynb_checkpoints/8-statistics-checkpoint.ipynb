{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A standard task in large-scale data processing, is to compute some statistics from a dataset. In the tutorial part, we will experiment on small samples, for the purpose of transparency in the processing. In the exercises, we will tackle a large dataset to make processing a bit more realistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = sc.parallelize([('M', 25), \n",
    "                       ('M', 20), \n",
    "                       ('M', 30), \n",
    "                       ('F', 25),\n",
    "                       ('F', 20),\n",
    "                       ('M', 30)])\n",
    "values = data.map(lambda x:x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few basic operations, such as **count**ing the number of elements, taking the **sum** and the **mean** (average) of values. Since the mean is equal to the sum divided by the number of elements, sum/count will also give the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "values.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "values.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "values.sum()/data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard descriptive statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also standard functions on RDDs to compute the standard deviation (average difference to the mean) and to distribute the values over buckets to prepare the data to plot in a histogram. For the standard deviation there are two functions, when working with sampled data (i.e. if we cannot observe all cases), the **sampleStddev** is more appropriate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "values.sampleStdev()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To plot a histogram, the data is distributed over equally sized buckets, and for every bucket the frequency represents the number of values that fall into that bucket. When we distribute the data over two equally sized buckets, the first list gives the bucket boundaries (20-25) and (25-30), and the second list the frequencies, e.g. 2 values fall into the (20-25) bucket (for all buckets except the last the upper bound is exclusive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "values.histogram(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For aggregated counts over keys and values, you can use the **countByKey** action on (key, value) pairs, and the **countByValue** action over elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.countByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "values.countByValue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to do a different computation over keys, then the  **reduceByKey**, **aggregateByKey**, **combineByKey** and **groupByKey** are the transformation function that allow you to do that. When the sum, min or max over values are required, reduceByKey offers the most straightforward solution. But for more complex function the other transformations provide all the tools you need. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = data.reduceByKey(lambda x, y: max(x, y))\n",
    "print(results.collect())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
